\chapter{Related work}


\section{Appearance descriptors}
Previous work focus on find more discriminative descriptors and better metric learning. A good descriptor is robust to problems like illumination, low resolution and viewpoint, etc.
To model the complex human kinematics, the part-based models are most adopted since human body is non-rigid body. Previous literature mainly contains three kinds of models [1], fixed part models, adaptive part models and the learned part models. The fixed part models are used in [2,3,4], where a silhouette is divided into a fixed number of horizontal and equal stripes, which mainly include head, torso, legs. In [9] the width of each stripe are respectively 16\%,29\% and 55\%.  The fixed models predefine the parameters like numbers of stripes and the stripe width. 

In the adaptive part models, the models vary from one to one according to predefined algorithm. Take [6] for an instance, the silhouette of each person is divided into three parts horizontally, which include the head, torso and legs respectively. But the width of each stripe is different for various silhouettes, and it is computed according to the symmetry and asymmetry with two operators $C(y, \sigma) $and $S(y,\sigma)$, where 
$$C(y,\sigma) = \sum{d^2(p_i-{\hat{p}_i)}}$$
$$S(y,\sigma) = \sum{\frac{1}{W\delta}|A(B[y,y-\delta]) - A(B[y,y+\delta])|}$$
Here the $C(y, \sigma) $ computes the asymmetry of two blobs and $S(y,\sigma)$ computes the difference of two areas. Then the axis between torso and legs are computed as follow
$$y_{TL} = \mathop{\arg\min}(1-C(y,\sigma)+S(y,\sigma))$$
and the axis  between head and torso is computed with following equation,
$$y_{HT} = \mathop{\arg\min}(-S(y,\sigma))$$
the axis divides the left and right torso is
$$ j_{LR} = \mathop{\arg\min}(C(y,\sigma)+S(Y,\sigma))$$

With those equations above axis can be computed depend on specific image. This method has a relatively high performance.

%-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
A work flow of people re-identification is shown in the figure []. Passers-by are detected and tracked with suitable algorithms. We can get bounding boxes containing walking people. The next step is to extract descriptors from the bounding boxes. To decrease the error brought by the background pixels we subtract the background pixels from the bounding boxes. With the silhouettes descriptors can be extracted based on various methods.  

With the extracted descriptors re-ID can be summarized as a similarity score computing problem. Classical people re-identification mainly deals with two steps, extracting the individual descriptor and comparing the similarity of different descriptors. Suppose there are two sets, probe set  and gallery set . In the probe set , there are a few descriptors representing individuals with unknown ID, while gallery set  consists of descriptor with known and labelled IDs. For each image in the probe set we compute its similarity score with every gallery descriptor. Then a list of the similarity scores are listed and the rank-1 descriptor's ID is believed to be probe's ID.


Besides, to model the complex human kinematics, the part-based models are adopted. Previous literature mainly contains three kinds of models[ ],fixed part models, adaptive part  models and the learned part models. The fixed part models are used in [  ], where a silhouette is divided into a fixed number of horizontal and equal stripes, which mainly include head, torso, legs. In [ ] the width of each stripe are respectively 16\%,29\% and 55\%.  The fixed models predefine the parameters like numbers of stripes and the stripe width.

In the adaptive part models, the models vary from one to one according to predefined algorithm. Take [ ] for an instance, the silhouette of each person is divided into three parts horizontally, which include the head, torso and legs respectively. But the width of each stripe is different for various silhouettes, and it is computed according to the symmetry and asymmetry with two operators  and ,where


The  computes the asymmetry of two blobs and  computes the difference of two areas. Then the axis between torso and legs are computed as follow,

and the axis between head and torso is computed with following equation,

the axis divides the left and right torso is 

With those equations above axis can be computed depend on specific image.

The part-based adaptive spatial-temporal model used in [12] characterizes person's appearance using color and facial feature. Few work exploits human face feature but in this work human face selection based on low resolution cues select useful face images to build face models. Color features capture representative color as well as the color distribution to build color model. This model handles multi-shots re-identification and it also model the color distribution variation of many consecutive frames.  Besides, the facial features of this model is conditional, that is, in the absence of good face images this model is only based on color features.

Some methods based on learned part models have been proposed. Part model detectors, that is, statistic classifiers, are trained with manully labelled human body parts images, exploiting features related with edges contained in the images. The pictorial structure is proposed in [4], and a PS model of a non-rigid body is a collection of part models with deformable configurations and connections with certain parts. The appearance of each part is separately modeled and deformable configurations are implemented with spring-like connections. This model can quantatively describe visual appearance and model the non-rigid body. In [ ] the body model is made up of N parts and N corresponding part detectors. Suppose  be the possible configurations of each part,where  is the state of the i-th body part and , and  are the coordinates of the part center,  is the absolute part orientation, and  is the scale size relative to the part size in the training set.  Given the image evidence D, the problem is to maximise the posterior  probability  that the part configuration is correct, and we have , where  is the likelihood of image evidence given a particular part configuration and  corresponds to a kinematic prior, and those two items can be learned from a training set.

Another example of learned part model is in [15,16], the overall human body model consists of several part models, each model is made up of a spatial model and a part filter. For each part the spatial model define allowded arrangements of this part with respect to the bounding box. To train each model the Latent Support Vector Machine is used and in [ 15,16 ] four body parts are detected, namely head, left and right torso and upper legs.

-'-'
The implement of every model of different model parts are finished by features. The features can be divided into two categories, the local and global feature. For a specified image or region, to extract the local feature we first divide the whole image into many equal blocks and compute the feature of each block. While the global feature refers to the feature extracted from a whole image or region, and the size of the descriptor is usually fixed. 

Global color histogram is a frequently used global feature. For an three-channel image, like RGB image, each channel is quantised into  bins separately. The final histogram could be a multi-dimensional or mono-dimensional histogram. For instance, if , for multi-dimensional histogram there will be  bins, but if we concatenate the 3 dimensional bins together the dimension can be reduced to  bins. Also this method can be applied on other color spaces like HSV and Lab, etc.

Local color histogram usually splits the specified model or region into many equal size blocks and compute the global feature of each block. The feature can be based on color, texture and interest points. SIFT[ ] is a kind of local feature based on the interest points. The salient interest points (identifiable over rotating and scaling) are selected by the interest operator. This algorithm detects the scale-extrema of the function difference of Gaussian function with scale , that is, 

 is the Gaussian filter with standard deviation 1*,  is the image, when the  image is computed, each point  is compared with its neighbors to get  extrema.

MSCR (maximally stable color region) is used in [ ]. The MSCR is derived from MSER (maximally stable extreme region), it detects the region with stable color. It uses an agglomerative clustering algorithm to compute color clusters, and by looking at the successive time steps of the algorithm the extension of color is implemented. The detected color region is described with a nine dimensional vector containing the area, averaging color, centroid and second moment matrix. With this vector the color region detected is easy to do scale and affine transforms.

RHSP (Recurrent Highly-Structured Patches) is used in [ ]. This feature captures patches with highly recurrent color and texture characteristics from extracted silhouette pixels. This feature is extracted with following steps, first random and probably overlapping small image patches are extracted from silhouette pixels. Then to capture those patches with informative texture the entropy of each patch (the sum of three channels' entropy) is computed, we discard those patches with entropy smaller than a specified threshold. In the next step some transforms are performed on the remaining patches to select those remain invariant to the transforms. Subsequently, the recurrence of each patch is evaluated with the LNCC(local normalized cross correlation) function. This evaluation is only performed on small region containing the patch instead of the whole image. Then the patches with high recurrence is clustered to avoid patches with similar content. Finally, the Gaussian cluster is applied to maintain the patch nearest to cluster's centroid for each cluster.
 
Researchers found that descriptors based on a single attribute are not robust to various datasets. That is, none of results from those descriptors outperforms other methods when tested on all datasets. A single structured descriptor can have only superior performance in a specified dataset but performs worse on other datasets. So combinations of different descriptors are exploited to improve the performance. 

--------------------The discussion starts here
Another...



Recently a new 3-D descriptor model is used to represent the individual. Compared with those 2-D models, this model exploits the depth data to help create the descriptor.

For the multi-shots re-ID, there are some additional methods to mention. ------- 
Convolutional neural network

In [ ] the authors exploit the spatial and temporal information.


The second one is to design the similarity computing methodology. That is, the way to compare how different two descriptors are. Previous method including the Euclidean distance, Bhattacharyya distance and Mahalanobis distance. The Euclidean distance are mostly used for the straightforward descriptors like color and texture descriptors. 

Finally, for the performance measures, for the closed set re-ID problem, the mostly used is the cumulative matching curve(CMC). The CMC curve describes the probability of right corresponse given a list of computed similarity score, and the first ranked ID is matched as the corresponding ID. For the open-set re-ID problem, re-ID accuracy and FAR(false accept rate) are adopted. The re-ID accuracy is the number of probe IDs that are correctly accepted, which is expresses as true positives(TP). The FAR is expresses as the mismatches(MM) and false positives(FP). The mismatches are those probe IDs that is incorrectly matched to the galley while in fact those probe IDs exist in the gallery. The false positive is those probe IDs incorrectly matched to the gallery while they don't exist in the gallery actually.


%--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


The part-based adaptive spatial-temporal model used in [7] characterizes person's appearance using color and facial feature. Few work exploits human face feature but in this work human face selection based on low resolution cues selects useful face images to build face models. Color features capture representative color as well as the color distribution to build color model. This model handles multi-shots re-identification and it also model the color distribution variation of many consecutive frames.  Besides, the facial features of this model is conditional, that is, in the absence of good face images this model is only based on color features.

Some methods based on learned part models have been proposed. Part model detectors, that is, statistic classifiers, are trained with manually labelled human body parts images, exploiting features related with edges contained in the images. The pictorial structure is proposed in [8], and a PS model of a non-rigid body is a collection of part models with deformable configurations and connections with certain parts. The appearance of each part is separately modelled and deformable configurations are implemented with spring-like connections. This model can quantitatively describe visual appearance and model the non-rigid body. In [8] the body model is made up of $N$ parts and $N$ corresponding part detectors. Suppose $L={(l_0,l_1,\ldots,_{N-1})}$ be the possible configurations of each part, where $l_i$ is the state of the i-th body part and $l_0=(x_i,y_i,\theta_i,s_i)$, $x_i$ and $y_i$ are the coordinates of the part center, $\theta_i$ is the absolute part orientation, and $s_i$ is the scale size relative to the part size in the training set.  Given the image evidence D, the problem is to maximize the posterior probability $P(D)$ that the part configuration is correct, and we have $P(D) \propto {P(L)*P(L)}$, where $P(L)$ is the likelihood of image evidence given a particular part configuration and P(L) corresponds to a kinematic prior, and those two items can be learned from a training set.

Another example of learned part model is in [12,13], the overall human body model consists of several part models, each model is made up of a spatial model and a part filter. For each part the spatial model define allowed arrangements of this part with respect to the bounding box. To train each model the Latent Support Vector Machine is used and in [12,13 ] four body parts are detected, namely head, left and right torso and upper legs. Compared with other models this model exploits a sequence of frames of an individual and thus captures appearance characteristics as well as the appearance variation over time.

Moreover, 3-D model is proposed to improve re-ID performance. A new 3-D model model called SARC3D [16] is used to represent the individual. Compared with those 2-D models, this model combines the texture and color information with their location information together to get a 3D model. This model starts with an approximate body model with single shape parameter. By precise 3-D mapping this parameter can be learned and trained with even few images (even one image is feasible). This model's construction is driven by the frontal, top and side views extracted from various videos, and for each view the silhouette of people is extracted to construct the 3-D graphical model. The final body model is sampled to get a set of vertices from previously learned graphic body model. Compared with other model, this model has a robust performance when dealing with partial occlusion, people pose and viewpoint variations since the model is based on people silhouettes from three viewpoints.

As for the feature for each model (a whole model or part-based model), the feature can be implemented with different methods. The features can be divided into two categories, the global and local feature. The global feature refers to the feature extracted from a whole image or region, and the size of the descriptor is usually fixed. While to extract the local feature of a specified image or region, we first divide the whole image into many equal blocks and compute the feature of each block.  Both descriptors may deal with color, texture and shape. The color is exploited most as the color histogram within different color space. descriptor based on texture, such as the SIFT, SURF and LBP are also widely combined to improve the performance.

Global color histogram is a frequently used global feature. For an three-channel image, like RGB image, each channel is quantized into $B$ bins separately. The final histogram could be a multi-dimensional or mono-dimensional histogram. For instance, if $B=8$, for multi-dimensional histogram there will be $8*8*8=512$ bins, but if we concatenate the 3 dimensional bins together the dimension can be reduced to $8+8+8=24$ bins while the performance of this reduced descriptor doesn't decrease. This method can be applied on other color spaces like HSV and Lab, etc.

Local color histogram usually splits the specified model or region into many equal size blocks and compute the global feature of each block. The feature can be based on color, texture and interest points. SIFT[14] is a kind of local feature based on the interest points. The salient interest points (identifiable over rotating and scaling) are selected by the interest operator. This algorithm detects the scale-extrema of the function difference of Gaussian function with scale $\sigma$, that is, 
$$D(x,y,\sigma) = (G(x,y,k_1\sigma)-G(x,y,k_2\sigma))*I(x,y)$$
the Gaussian filter with standard deviation $k1*\sigma$, $I(x, y)$ is the image, when the DoG image is computed, each point $(x, y)$ is compared with its neighbours to get  extrema.

MSCR (maximally stable color region) is also used in [6]. The MSCR is derived from MSER (maximally stable extreme region), it detects the region with stable color and uses an agglomerative clustering algorithm to compute color clusters, by looking at the successive time steps of the algorithm the extension of color is implemented. The detected color region is described with a nine dimensional vector containing the area, averaging color, centroid and second moment matrix. With this vector the color region detected is easy to do scale and affine transforms.

RHSP (Recurrent Highly-Structured Patches) is used in [6]. This feature captures patches with highly recurrent color and texture characteristics from extracted silhouette pixels. This feature is extracted with following steps, first random and probably overlapping small image patches are extracted from silhouette pixels. Then to capture those patches with informative texture the entropy of each patch (the sum of three channels' entropy) is computed, we discard those patches with entropy smaller than a specified threshold. In the next step some transforms are performed on the remaining patches to select those remain invariant to the transforms. Subsequently, the recurrence of each patch is evaluated with the LNCC(local normalized cross correlation) function. This evaluation is only performed on small region containing the patch instead of the whole image. Then the patches with high recurrence is clustered to avoid patches with similar content. Finally, the Gaussian cluster is applied to maintain the patch nearest to cluster's centroid for each cluster.
 
Combined descriptors are found to have better performance. Descriptors combining color and texture are most often used in re-identification. In [5] a signature called asymmetry-based histogram plus epitome(AHPE) was proposed. This work starts with a selection of images to reduce image redundancy (redundancy is caused by correlated consecutive sequences). This descriptor combines global and local statistical descriptors of human appearance, focusing on overall chromatic content via histogram and on the recurrent local patches via epitome analysis [6]. Similar to SDALF descriptor [6], HPE descriptor consists of three components, the chromatic color histogram, the generic epitome and local epitome. The chromatic color histogram is extracted in the HSV color space, which turns to be robust to illumination changes. Here color histogram is encoded into a 36-dimensional feature space $[H=16,  S=16,  V=4]$. Besides, the authors customize the use of epitome here by extracting generic and local epitome here. 

\section{Metric learning}
The second step of re-ID is to design the similarity computing methodology to compare descriptors. That is, the way to compare how different two descriptors are. This is also call the metric learning. Generally, for two input vectors $x_1$, $x_2$, any symmetric positive semi-definite matrix W defines a pseudo-metric with the form of $D = x_1*W*x_2$. Many widely used distance metric obey this rule. Previous methods includes the Euclidean distance, Bhattacharyya distance and Mahalanobis distance. The Euclidean distance, which is mostly used for the straightforward descriptors like color and texture descriptors,  is one case of the $L_p$ distance when $p=2$, and also a special case of Mahalanobis distance when the covariance matrix of two input observations is an identity matrix. One example of metric learning is the probabilistic relative distance comparison model proposed in [4]. This model decreases the error caused by sometimes high intra-class variation and low inter-class variation. Compared with other distance learning models proposed, this model behaves more robust for its probability relative distance comparison model. Suppose z is an image of a person, the task is to identify another image $z'$ of the same person from $z''$ of a different person by using a distance model $f(.,.)$ so that $f(z, z')< f(z, z'')$. The contribution of this paper is that the author transfers the distance learning problem into a probability comparison problem by measuring the probability of distance between a relevant pair of images being smaller than that of a related irrelevant pair as
\begin{equation}P(f(z,z')<f(z,z'')) = (1+e^{(f(z-z')-f(z-z''))})^{-1}\end{equation}
Here the author assumes the probability of $f(z,z')$ and $f(z,z'')$ is independent, therefore, using maximal likelihood principal the optimal function can be learned as

\begin{equation}
%\begin{aligned}
f = \mathop{\arg\min}_f r(f,O) 
r(f,O) = -log(\Pi_{O_i}P(f(z-z')-f(z-z'')))
%\end{aligned}
\end{equation}
$O=\{O_i=(x_i^p-x_i^n)\}$ , $x_i^p$,$x_i^n$ are the pair from same person and different person respectively.
The distance function $f(\cdot)$ here is parameterized as Mahalanobis distance function
$f=\bm{x}^T\bm{M}\bm{x},\bm{M}\ge0$,
here \textbf{M} is a semi-definite matrix, in this way the distance function learning problem is transformed to a matrix learning problem. The author used an iteration algorithm to compute matrix $M$.

Since still image based person representation suffers from factors like illumination, occlusion, viewpoint change and pose difference. The multi-shot re-ID has been proposed. Since there are a sequence of images for each indivudual, there are much more cues to exploit.
In [TDL], the author simplified computing of Mahananobis matrix by applying the new limitations on datasets. The author finds that when using video based person representation the difference of inter-class may be more obscure than that of still image based representation. Therefore, the author proposed the top-push distance learning. For a person video sequence, the maximal intra-class distance should be smaller than the minimal distance of inter-class distance. One another requirement is the sum of all intra-class distance should be as small as possible, so the final target function is summarized as 

\begin{equation}
%\begin{aligned}
f(D) = (1-\alpha)\sum_{x_i,x_j,y_i=y_j} D(x_i,x_j) +  \alpha \sum_{x_i,x_j,y_i=y_j}\max\{{D(x_i,x_j)-\min_{y_i\ne y_k}{D(x_i,x_k)}+\rho,0}\}
%\end{aligned}
\end{equation}

Cross view quadratic analysis(XQDA) is proposed in [xqda paper]. In [36 of lomo paper] Mogaddam et al. proposed to model each of two classes with a multivariate Gaussian  distribution. Suppose the sample difference $\Delta = x_i - x_j$, where $x_i$ and $x_j$ are two feature vectors. $\Delta$ is called intrapersonal difference when their label $y_i = y_j$ and extrapersonal difference when $y_i \ne y_j$. Respectively two the intrapersonal and interpersonal variation can be defined as $\Omega_I$ and $\Omega_E$,


Besides those aforementioned metric learning, some metric learning by neural network also draws much interest. That is, to define a neural network to compute similarity of two input descriptors or even images. Recently the deep neural network has been exploited to improve the performance. One advantage of neural network re-ID is the preprocessing of images can be skipped (We can also say the preprocess is included in convolutional layers). The input of this structure can be straight-forward grey images or color images. To deal with multi-shots and video based re-identification neural network is proven to have better performance. But for the classical neural network there are too many weights to train and the over-fitting problem can be troublesome. Convolutional neural network can avoid those problems while remains high performance. Compared with classical neural network architecture, the convolutional neural network exploits receptive field, weights sharing and pooling technology to reduce weights number and thus decreases computational cost. In [11] for the first time the author proposes a recurrent neural network layer and temporal pooling to combine all time-steps data to generate a feature vector of the video sequence. In [17] the author proposes a multi-channel layers based neural network to jointly learn both local body parts and whole body information from input person images.  In [18] a convolutional neural network learning deep feature representations from multiple domains is proposed, and this work also proposes a domain guided dropout algorithm to dropout CNN weights when learning from different datasets. This method gives a solution to the problem that most CNNs are not trained enough caused from datasets with small number of images since this CNN learns from multiple datasets.