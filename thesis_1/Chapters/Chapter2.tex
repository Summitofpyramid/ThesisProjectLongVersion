\chapter{Related work}
Previous work focus mainly focus on finding more discriminative descriptors and better metric learning. It's known that color and texture are the most important information in Re-ID. Most descriptors captures the local or global statistic color and texture information to characterize individuals. A brief introduction of those descriptors are given in this chapter.

\section{Appearance descriptors}
In most descriptors, the input image will be divided into a few subregions to model the complex human kinematics. Features of those subregions are extracted respectively and concatenated directly or characterized by their statistic properties. According to how those subregions are divided, there can be three kind of models, fixed-part based models, adaptive models and learned part models \cite{Appearancedesc}. 

In fixed part models, the size of body parts are predefined. One example is in \cite{ImportantFeatures, PRDC, REIDSVM}, where a silhouette is divided into a fixed number of horizontal and equal stripes, which mainly include head, torso, legs. In \cite{AppBasedREID} the input image are divided into three horizontal stripes and widths of each stripe are respectively 16\%, 29\% and 55\%.  The fixed models predefine the parameters like numbers of stripes and the stripe width. 

In the adaptive part models, the size of each body parts may vary to fit predefined body part models. Take \cite{SDALF} for an instance, the silhouette of each person is divided into three parts horizontally, which include the head, torso and legs respectively. But the width of each stripe is different for various silhouettes, and it is computed according to the symmetry and asymmetry with two operators $C(y, \sigma) $and $S(y,\sigma)$, where 
\begin{equation}
\begin{aligned}
C(y,\sigma) & = \sum{d^2(p_i-{\hat{p}_i)}} \\
S(y,\sigma) &= \sum{\frac{1}{W\delta}|A(B[y,y-\delta]) - A(B[y,y+\delta])|}
\end{aligned}
\end{equation}
Here the $C(y, \sigma)$ computes the asymmetry of two blobs and $S(y,\sigma)$ computes the difference of two areas. Then the axis between torso and legs are computed as follow
\begin{equation}
\begin{aligned}
y_{TL} = \mathop{\arg\min}(1-C(y,\sigma)+S(y,\sigma))
\end{aligned}
\end{equation}
and the axis  between head and torso is computed with following equation,
\begin{equation}
\begin{aligned}
y_{HT} = \mathop{\arg\min}(-S(y,\sigma))
\end{aligned}
\end{equation}
the axis divides the left and right torso is
\begin{equation}
\begin{aligned}
j_{LR} = \mathop{\arg\min}(C(y,\sigma)+S(y,\sigma))
\end{aligned}
\end{equation}
This method has a relatively high performance. But one shortcoming of this model is imperfect background segmentation causes noise and errors to the axis' position. \\
%-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\indent Another part-based adaptive spatial-temporal model used in \cite{PartbasedSTReid} characterizes person's appearance using color and facial feature. Few work exploits human face feature but in this work human face selection based on low resolution cues select useful face images to build face models. Color features capture representative color as well as the color distribution to build color model. This model handles multi-shots re-identification and it also model the color distribution variation of many consecutive frames.  Besides, the facial features of this model is conditional, that is, in the absence of good face images this model is only based on color features.

Some methods based on learned part models have been proposed. Part model detectors (statistic classifiers) are trained with manually labelled human body parts images, exploiting features related with edges contained in the images. The pictorial structure is proposed in \cite{PictorialModel}, and a PS model of a non-rigid body is a collection of part models with deformable configurations and connections with certain parts. The appearance of each part is separately modelled and deformable configurations are implemented with spring-like connections. This model can quantitatively describe visual appearance and model the non-rigid body. In \cite{PSmodelRevisit} the pictorial structure body model is made up of N parts and N corresponding part detectors. 

Another example of learned part model is in \cite{MultiPersonREID, PartbasedSTReid}, the overall human body model consists of several part models, each model is made up of a spatial model and a part filter. For each part the spatial model defines allowed arrangements of this part with respect to the bounding box. To train each model the Latent Support Vector Machine is used and four body parts are detected, namely head, left and right torso and upper legs. Compared with other models this model exploits a sequence of frames of an individual and thus captures appearance characteristics as well as the appearance variation over time.

According to the way to extract feature for each model (a whole model or part-based model), the feature can be implemented with different methods. The features can be divided into two categories, the global and local feature. The global feature refers to the feature extracted from a whole image or region, and the size of the descriptor is usually fixed. While to extract the local feature of a specified image or region, we first divide the whole image into many equal blocks and compute the feature of each block.  Both descriptors may deal with color, texture and shape. The color is exploited most as the color histogram within different color space. descriptor based on texture, such as the SIFT, SURF and LBP are also widely combined to improve the performance.

Global color histogram is a frequently used global feature. For an three-channel image, like RGB image, each channel is quantized into $B$ bins separately. The final histogram could be a multi-dimensional or mono-dimensional histogram. For instance, if $B = 8$, for multi-dimensional histogram there will be  $8\times 8\times 8 = 512$ bins, but if we concatenate the 3 dimensional bins together the dimension can be reduced to $8 + 8 + 8 = 24$ bins while the performance of this reduced descriptor doesn?t decrease. This method can be applied on other color spaces like HSV and Lab, etc.

Local color histogram usually splits the specified model or region into many equal size blocks and compute the global feature of each block. The feature can be based on color, texture and interest points. SIFT \cite{SIFT} is a kind of local feature based on the interest points. The salient interest points (identifiable over rotating and scaling) are selected by the interest operator. This algorithm detects key points by computing $DoG$ image of different scale $\sigma$ with equation
\begin{equation}
D(x,y,\sigma) = (G(x,y,k_1\sigma) - G(x,y,k_2\sigma))\ast I(x,y)
\end{equation}
here $G(x,y,k_1\sigma)$ is the gaussian function with deviation $k_1\sigma$, $I(x,y)$ is the image. The $DoG$ images are compared to find their extrema as key points. With key points localization and other processing, descriptors describing key points are created as SIFT descriptors.

Maximally stable color region (MSCR) is used in \cite{SDALF}. The MSCR derives from MSER (maximally stable extreme region) and detects the region with stable color cluster. It uses an agglomerative clustering algorithm to compute color clusters, and by looking at the successive time steps of the algorithm the extension of color is implemented. The detected color region is described with a nine dimensional vector containing the area, averaging color, centroid and second moment matrix. With this vector the color region detected is easy to do scale and affine transforms.

Recurrent highly-structured patches (RHSP) is also used in \cite{SDALF}. This feature captures patches with highly recurrent color and texture characteristics from extracted silhouette pixels. This feature is extracted with following steps, first random and probably overlapping small image patches are extracted from silhouette pixels. Then to capture those patches with informative texture the entropy of each patch (the sum of three channels' entropy) is computed, we discard those patches with entropy smaller than a specified threshold. In the next step some transforms are performed on the remaining patches to select those remain invariant to the transforms. Subsequently, the recurrence of each patch is evaluated with the LNCC(local normalized cross correlation) function. This evaluation is only performed on small region containing the patch instead of the whole image. Then the patches with high recurrence is clustered to avoid patches with similar content. Finally, the Gaussian cluster is applied to maintain the patch nearest to cluster's centroid for each cluster.
 
Researchers found that descriptors based on a single attribute are not robust to various datasets. That is, none of results from those descriptors outperforms other methods when tested on all datasets. A single structured descriptor can have only superior performance in a specified dataset but performs worse on other datasets. So combinations of different descriptors are exploited to improve the performance. 

Moreover, 3-D model is proposed to improve Re-ID performance. A new 3-D model model called SARC3D \cite{SARC3D} is used to represent the individual. Compared with those 2-D models, this model combines the texture and color information with their location information together to get a 3D model. This model starts with an approximate body model with single shape parameter. By precise 3-D mapping this parameter can be learned and trained with even few images (even one image is feasible). This model's construction is driven by the frontal, top and side views extracted from various videos, and for each view the silhouette of people is extracted to construct the 3-D graphical model. The final body model is sampled to get a set of vertices from previously learned graphic body model. Compared with other model, this model has a robust performance when dealing with partial occlusion, people pose and viewpoint variations since the model is based on people silhouettes from three viewpoints.

Besides, for the performance measures, for the closed set Re-ID problem, the mostly used is the cumulative matching curve(CMC). The CMC curve describes the probability of right match given a list of computed similarity score, and the first ranked ID is regarded as the matched individual. For the open-set Re-ID problem, e-ID accuracy and FAR(false accept rate) are adopted. The Re-ID accuracy is the number of probe IDs that are correctly accepted, which is expresses as true positives(TP). The FAR is expresses as the mismatches(MM) and false positives(FP). The mismatches are those probe IDs that is incorrectly matched to the galley while in fact those probe IDs exist in the gallery. The false positive is those probe IDs incorrectly matched to the gallery while they don't exist in the gallery actually.
%--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Combined descriptors are found to have better performance. Descriptors combining color and texture are most often used in re-identification. In \cite{AppBasedREID} a signature called asymmetry-based histogram plus epitome(AHPE) was proposed. This work starts with a selection of images to reduce image redundancy (redundancy is caused by correlated consecutive sequences). This descriptor combines global and local statistical descriptors of human appearance, focusing on overall chromatic content via histogram and on the recurrent local patches via epitome analysis. Similar to SDALF descriptor \cite{SDALF}, HPE descriptor consists of three components, the chromatic color histogram, the generic epitome and local epitome. The chromatic color histogram is extracted in the HSV color space, which turns to be robust to illumination changes. Here color histogram is encoded into a 36-dimensional feature space $[H=16,  S=16,  V=4]$. Besides, the authors customize the use of epitome here by extracting generic and local epitome here. 

\section{Metric learning}
Many different metric learning methods have been proposed \cite{KISSME, LFDA, PCCA, TDL, PRDC, LMNN, KLFDA, KCCA, KernelVersionMetrics, NFST, ITML} to get smaller intraclass distance and larger interclass distance. And Mahanalobis distance is adopted to 
The second step of Re-ID is to design the metric learning to match descriptors. That is, the way to compare how similiar two descriptors are. Generally, for two $d\times 1$ dimensional input vectors $\bm{x}_1$, $\bm{x}_2$, any symmetric positive semi-definite matrix $M$ defines a pseudo-metric with the form of $D = (\bm{x}_1 -\bm{x}_2)\bm{M}(\bm{x}_1 - \bm{x}_2)$. Many widely used distance metric exploit this rule. Previous methods includes the Euclidean distance, Bhattacharyya distance and Mahalanobis distance. The Euclidean distance, which is most common used distance, is a special case of Mahalanobis distance when the $\bm{M}$ is an identity matrix. One example of metric learning is the probabilistic relative distance comparison model proposed in \cite{PRDC}. This model  all the possible positive person pairs with those negative pairs so that the distance of between-class distance is larger than the distance of within-class distance. Compared with other distance learning models proposed, this model solves the matrix $M$ by an iterative optimization algorithm. Suppose z is an image of a person, the task is to identify another image $z'$ of the same person from $z''$ of a different person by using a distance model $f(\cdot)$ so that $f(z, z')< f(z, z'')$. The authors convert the distance learning problem to a probability comparison problem by measuring the probability of distance between a relevant pair of images being smaller than that of a related irrelevant pair as
\begin{equation}P(f(z,z')<f(z,z'')) = (1+e^{(f(z-z')-f(z-z''))})^{-1}\end{equation}
Here the author assumes the probability of $f(z,z')$ and $f(z,z'')$ is independent, therefore, using maximal likelihood principal the optimal function can be learned as
\begin{equation}
f = \mathop{\arg\min}_f r(f,O) 
r(f,O) = -log(\Pi_{O_i}P(f(z-z')-f(z-z'')))
\end{equation}
$O=\{O_i=(x_i^p-x_i^n)\}$ , $x_i^p$, $x_i^n$ are the pair from same person and different person respectively.
The distance function $f(\cdot)$ here is parameterized as Mahalanobis distance function
$f=\bm{x}^T\bm{M}\bm{x},\bm{M}\ge0$,
here \textbf{M} is a semi-positive definite (SPD) matrix, in this way the distance function learning problem is transformed to a matrix optimization problem. The author used an iteration algorithm to compute matrix $\bm{M}$. One shortcoming for this algorithm is it's computationally expensive because for each person it compares all the possible negative pair distance with corresponding negative pair distance. 

Single shot image based person representation suffers from small sample size problem. The multi-shot Re-ID has been proposed. Since there are a sequence of images for each individual, there are much more cues to exploit.
In \cite{TDL}, the author simplified computing of Mahananobis matrix by applying the new limitations on datasets. The author finds that when using video based person representation the difference of inter-class may be more obscure than that of still image based representation. Therefore, the author proposed the top-push distance learning. For a person video sequence, the maximal intraclass distance should be 1 unit smaller than the minimal distance of interclass distance. Another limitation of this work is the sum of all intra-class distance should be as small as possible, so the final target function is summarized as 
\begin{equation}
\begin{aligned}
f(D) = (1-\alpha)\sum_{x_i,x_j,y_i=y_j} D(x_i,x_j) + \\
\alpha \sum_{x_i,x_j,y_i=y_j}\max\{{D(x_i,x_j)-\min_{y_i\ne y_k}{D(x_i,x_k)}+\rho,0}\}
\end{aligned}
\end{equation}

\section{Other methods for Re-ID}
Beside descriptors and metrics mentioned above, there are some other methods for Re-ID. Convolutional neural network have been exploited in Re-ID. One advantage of neural network Re-ID is the preprocessing of images can be skipped (We can also say the preprocessing is included in convolutional layers). The input of this structure can be straight-forward grey images or color images. To deal with multi-shots and video based re-identification neural network is proven to have better performance. Traditional neural network has too many weights to train. Convolutional neural network can avoid this problem while retaining high performance. Compared with classical neural network architecture, the convolutional neural network exploits receptive field, weights sharing and pooling technology to reduce weights number and thus decreases computational cost. In \cite{RecurrentCNN} the author proposes a recurrent neural network layer and temporal pooling to combine all time-steps data to generate a feature vector of the video sequence. In \cite{MultiCNN} the author proposes a multi-channel layers based neural network to jointly learn both local body parts and whole body information from input person images.  In \cite{DeepfeatureCNN} a convolutional neural network learning deep feature representations from multiple domains is proposed, and this work also proposes a domain guided dropout algorithm to dropout CNN weights when learning from different datasets. \\
\indent There are many other works based on convolutional neural networks. However, person re-identification may be one of the area which CNN won't work for the small sample size(SSS) problem. In most datasets, the sample size of each pedestrian is quite small. Especially in single shot Re-ID only one frame is provided in each view for each person. So Re-ID will more rely on classical machine learning.

%
%By learning from multiple datasets this method gives a solution to the problem that most CNNs are not trained enough caused by datasets with small number of images.

\section{Some state-of-the-art works}
Recently many works have been proposed and improved Re-ID performance by much margin. In this section those advanced descriptors and metrics are introduced. 
\textbf{Cross view quadratic discriminant analysis}(XQDA) is proposed in \cite{LOMO}. Suppose the sample difference $\Delta = \bm{x}_i - \bm{x}_j$, where $\bm{x}_i $ and $\bm{x}_j$ are two feature vectors. $\Delta$ is called intrapersonal difference when their label satisfy $y_i = y_j$ and extrapersonal difference when $y_i \ne y_j$. Respectively two the intrapersonal and interpersonal variation can be defined as $\Omega_I$ and $\Omega_E$, the authors convert Re-ID problem to distinguish $\Omega_I$ and $\Omega_E$. In \cite{Bayeface}  each one of intrapersonal and interpersonal class is modelled with a multivariate gaussian distribution, and in \cite{Bayeface} it has been proved that both $\Omega_I$ and $\Omega_E$ have zero mean. Under the zero-mean distribution, the probability of observing $\Delta$ in $\Omega_I$ and the probability of observing $\Delta$ in $\Omega_E$ can be denoted as
\begin{equation}
\begin{aligned}
P(\Delta|\Omega_I) &= \frac{1}{(2\pi)^{d/2}|\Sigma_I|^{1/2}}\exp^{\frac{-1}{2}\Delta^T\Sigma_I^{-1}\Delta}\\
P(\Delta|\Omega_E) &= \frac{1}{(2\pi)^{d/2}|\Sigma_E|^{1/2}}\exp^{\frac{-1}{2}\Delta^T\Sigma_E^{-1}\Delta}
\end{aligned}
\end{equation}
where $\Sigma_I$ and $\Sigma_E$ are the covariance matrix of $\Omega_I$ and $\Omega_E$, then the probability ratio between the interpersonal pairs and intrapersonal pairs can be denoted as
\begin{equation}
\begin{aligned}
r(\Delta) = \frac{P(\Delta|\Omega_E)}{P(\Delta|\Omega_I)}
	   &=\frac{\frac{1}{(2\pi)^{d/2}|\Sigma_E|^{1/2}}\exp^{\frac{-1}{2}\Delta^T\Sigma_E^{-1}\Delta}}{\frac{1}{(2\pi)^{d/2}|\Sigma_I|^{1/2}}\exp^{\frac{-1}{2}\Delta^T\Sigma_I^{-1}\Delta}}\\
r(\Delta)& =  C\exp^{\frac{-1}{2}\Delta^T(\Sigma_E^{-1} - \Sigma_I^{-1})\Delta}
\end{aligned}
\end{equation}
C is the constant term, by taking log and deserting the constant term, we have 
\begin{equation}
\begin{aligned}
r(\Delta) = \Delta^T(\Sigma_I^{-1} - \Sigma_E^{-1})\Delta 
	   & = (\bm{x}_i - \bm{x}_j)^T(\Sigma_I^{-1} - \Sigma_E^{-1})(\bm{x}_i - \bm{x}_j)
\end{aligned}
\end{equation}
In \cite{LOMO} a subspace $W$ is learned so that 
\begin{equation}
r(\Delta) = (\bm{x}_i - \bm{x}_j)^TW({\Sigma_I}'^{-1} - {\Sigma_E}'^{-1})W^T(\bm{x}_i - \bm{x}_j)
\end{equation}
and ${\Sigma_I}' = W^T\Sigma_IW, {\Sigma_E}' = W^T\Sigma_EW$. Therefore, a subspace $M(W) = W({\Sigma_I}'^{-1} - {\Sigma_E}'^{-1})W^T$ is learned in this work.\\
\indent \textbf{Null Foley-Sammon transform} In \cite{NFST} a null space is proposed so that with this space the intraclass points collapse to a same point in the null space while interclass points are projected to different points. Given the within class scatter $\bm{S}^w$ and between class scatter $\bm{S}^b$, an optimal projection matrix $\bm{W}$ is computed so that 
\begin{equation}
\begin{aligned}
\bm{w}_i^T\bm{S}^w\bm{w}_i = 0\\
\bm{w}_i^T\bm{S}^b\bm{w}_i > 0
\end{aligned}
\end{equation}
$\bm{w}_i$ is the $i_{th}$ column in $\bm{W}$.

It can be noticed that like many other metric learnings, XQDA and NFST are transformed into matrix decomposition and eigenvalue selection problem. In this paper, those two metrics are used to compare with proposed metric. In \cite{GOG} it has been shown GOG + XQDA outperforms many other combinations including MetricEnsemble \cite{MetricEnsembles}, SCNCD \cite{SCNCD}, SemanticMethod \cite{SemanticMethod}, etc. In \cite{ NFST} it has been shown LOMO + NFST outperforms metrics including LMNN \cite{LMNN}, KCCA \cite{KCCA}, ITML \cite{ITML}, KLFDA\cite{KLFDA}, MFA \cite{KernelVersionMetrics}, KISSME \cite{KISSME}, SimilarityLearning \cite{SimilarityLearning}, SCNCD \cite{SCNCD}, Mid-level Filters \cite{MidlevelFilters} and Improved Deep \cite{ImprovedCNN}. Based on the result that XQDA and NFST outperform other metrics, in this thesis, XQDA and NFST are used to compare with proposed metric learning. 



