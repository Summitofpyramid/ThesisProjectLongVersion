\chapter{Dimension reduction and Mahanalobis distance learning}
To deal with high dimensional descriptors, dimension reduction are firstly performed by kernel local fisher discriminant analysis (KLFDA). Then Mahanalobis distance metric learning based on limitations between interclass and intraclass distance are operated on dimension reduced data.
\section{Kernel local fisher discriminant analysis}
The extracted hierarchical gaussian descriptors have high dimension, it's intractable to learn a SPD matrix with such a high dimension. Dimension reduction is required to learn a subspace.
Among those methods to reduce dimension, principal component analysis (PCA) is often used. However, PCA is an unsupervised dimension reduction and may have a low performance for those reasons, $(1)$, PCA is to maximize the variance of dimension reduced data, and as a unsupervised method it doesn't has a full consideration of the the relation of between and within classes, it is very likely that the descriptors of different classes can be mixed up after the dimension reduction; $(2)$ PCA may suffer from the small sample size problem. In some Re-ID datasets, there may be two or less images for each pedestrian in each viewpoint (like VIPeR), if the dimension of descriptor is much bigger than sample size, much information can be lost with PCA. In this thesis, the kernel local fisher discriminant analysis (KLFDA) is used to reduce dimension. 

KLFDA is the kernel version of LFDA, and LFDA is a combination of Fisher discriminant analysis \cite{LFDA} and and the locality preserving projection \cite{LPP} and kernel method. A brief introduction of FDA, LPP and kernel method is introduced below.
\subsection{Fisher discriminant analysis (FDA)}

FDA is a supervised dimension reduction and its input contains the class labels. For a set of $d$-dimensional observations $\bm{x}_i$, where $i\in\{1,2,\cdots,n\}$, the label $l_i\in\{1,2,\cdots,l\}$. Two matrix are defined as the intraclass scatter matrix $\bm{S}^{(w)}$ and between class scatter matrix
$\bm{S}^{(b)}$, 
\begin{equation}
\begin{aligned}
\bm{S}^{(w)} &= \mathop{\sum} _{i=1}^l\mathop{\sum}_{j:l_j = i} (\bm{x}_j - \bm{\mu}_i)(\bm{x}_j - \bm{\mu}_i)^T \\
\bm{S}^{(b)}  &= \mathop{\sum} _{i=1}^l n_i(\bm{\mu}_i - \bm{\mu})(\bm{\mu}_i - \bm{\mu})^T
\end{aligned}
\end{equation}
where the $\bm{\mu}_i$ is the mean of samples whose label is $i$, and $\bm{\mu}$ is the mean of all samples,
\begin{equation}
\begin{aligned}
\bm{\mu}_i &= \frac{1}{n_i} \sum \bm{x}_i, \\
\bm{\mu} &= \frac{1}{n} \sum \bm{x}_i
\end{aligned}
\end{equation}

The Fisher Discriminant Analysis transform matrix $\bm{T}$ can be represented as 
\begin{equation}
\bm{T} = \arg\max \frac{\bm{T}^T\bm{S}^{(b)}\bm{T}}{\bm{T}^T\bm{S}^{(w)}\bm{T}}
\end{equation}
This equation can be solved by Lagrange multiplier method, we define a Lagrange function 
\begin{equation}
L(\bm{t}) = \bm{t}^T\bm{S}^{(b)}\bm{t} - \lambda(\bm{t}^T\bm{S}^{(w)}\bm{t} - 1)
\end{equation}
Then the differential respect to $\bm{t}$ is 
\begin{equation}
\frac{\partial L(\bm{t})}{\partial \bm{t}} = 2\bm{S}^{(b)}\bm{t} - 2\lambda \bm{S}^{(w)}\bm{t}
\end{equation}

let 
\begin{equation}
\frac{\partial L(\bm{t})}{\partial \bm{t}} = 0
\end{equation}

we can get 
\begin{equation}\label{eigen1}
\bm{S}^{(b)}\bm{t}_i  = \lambda \bm{S}^{(w)}\bm{t}_i
\end{equation}


here $\bm{t}_i$ is the $i_{th}$ column of $\bm{T}$, and the optimization problem is converted to a eigenvalue decomposition problem. 

Fisher discriminant analysis tries to minimize the intraclass scatter matrix while maximize the interclass scatter matrix, and $\bm{T}$ is computed by the eigenvalue decomposition. $\bm{T}$ can be represented as the set of all the corresponding eigenvectors, as $ \bm{T} = (\bm{t}_1,\bm{t}_2,\cdots,\bm{t}_k)$.

FDA has a form similar with signal and noise ratio, however, the FDA dimension reduction may have poor performance for it doesn't consider the locality of data. An example of this is the multimodality \cite{KLFDA}. Multimodality is the case many clusters are formed in the same class. 
\subsection{Locality preserving projection (LPP)}

In \cite{LPP} locality preserving projection (LPP) is proposed to exploit data locality. An affinity matrix is created to record the affinity of sample $\bm{x}_i$ and $\bm{x}_j$,  typically the range of elements in $\bm{A}_{i,j}$ is $[0,1]$. There are many manners to define a $n \times n$ affinity matrix $\bm{A}$, usually two sample points with a smaller distance has a higher affinity value than those with bigger distance value. One of them is if  $\bm{x}_i$ is within k-nearest neighbours of $\bm{x}_j$ then $\bm{A}_{i,j} = 1$ otherwise  $\bm{A}_{i,j} = 0$.  

Another diagonal matrix $\bm{D}$ can be defined that each diagonal element is the sum of corresponding column in $\bm{A}$,
\begin{equation}
\bm{D}_{i,i} = \mathop{\sum}_{j=1}^n \bm{A}_{i, j} 
\end{equation}
then the LPP transform matrix is defined as follow,
\begin{equation}
\bm{T}_{LPP} = \mathop{\arg\min}_{\bm{T}\in\bm{R}^{d\times m}} \frac{1}{2}\mathop{\sum}_{i, j= 1}^n \bm{A}_{i,j} ||\bm{T}^T\bm{x}_i - \bm{T}^T\bm{x}_j||
\end{equation}
so that $ \bm{T}^T\bm{X}\bm{D}\bm{X}^T\bm{T} = \bm{I} $.
Suppose the subspace has a dimension of $m$, then LPP transform matrix $T$ can be represented as 

$$\bm{T}_{LPP} = \{ \bm{\phi}_{d-m+1} | \bm{\phi}_{d-m+2} | \cdots \bm{\phi}_{d}\} $$

And each $\bm{\phi}$ in $T$ is the eigenvector of following fomula,
 \begin{equation}
\bm{X}\bm{L}\bm{X}^T\bm{\phi} = \gamma\bm{X}\bm{D}\bm{X}^T
\end{equation}
where $\gamma$ is corresponding eigenvalue of $\bm{\phi}$, and $\bm{L} = \bm{D} - \bm{A}$.
%where $\bm{D}$ is the $n$-dimensional diagonal matrix with $i_{th}$ diagonal element 
%\begin{equation}
%\bm{D}_{i,i} = \sum_{j = 1}^n\bm{A}_{i,j}
%\end{equation}

%But the LPP dimension reduction is still not discriminant enough, 
\subsection{Local fisher discriminant analysis (LFDA)}
\indent LFDA \cite{LFDA} combines FDA and LPP and has better performance. The key in LFDA is it assigns weights to elements in $\bm{A}^{(w)}$ and $\bm{A}^{(b)}$, so that,
\begin{equation}
\begin{aligned}
\bm{S}^{(w)} &= \frac{1}{2}\sum _{i=1}^l\sum_{j:y_j = i} \bm{A}_{i,j}^w (\bm{x}_j - \bm{\mu}_i)(\bm{x}_j - \bm{\mu}_i)^T \\
\bm{S}^{(b)} &=  \frac{1}{2}\sum _{i=1}^l \bm{A}_{i,j}^b(\bm{\mu}_i - \bm{\mu})(\bm{\mu}_i - \bm{\mu})^T
\end{aligned}
\end{equation}
where 

\begin{equation}
\begin{aligned}
\bm{A}_{i,j}^{(w)} = \left \{ 
\begin{array}{rcl}
\bm{A}_{i,j}/n_c &  &y_i = y_j \\
0 & & {y_i \ne y_j }
\end{array}
  \right.  \\
  \bm{A}_{i,j}^{(b)} = \left \{ 
\begin{array}{rcl}
(\frac{1}{n} - \frac{1}{n_c})  \bm{A}_{i,j} &  &{y_i = y_j }\\
\frac{1}{n} & & {y_i \ne y_j }
\end{array}
  \right. 
 \end{aligned}
\end{equation}
where $y_i$ is the class label of sample point $\bm{x}_i$. So the transformation matrix $T_{LFDA}$ can be computed by equation
\begin{equation}\label{eigencompute1} 
\bm{T}_{LFDA}  = \arg\min_{\bm{T}} (\frac{\bm{T}^T\bm{S}^{(b)}\bm{T}}{\bm{T}^T\bm{S}^{(w)}\bm{T}})
\end{equation}
Again this problem can be solved by eigenvalue decomposition by equation \ref{eigen1}. 



When applying the LFDA to original high dimensional descriptors, one problem is the computation cost. Suppose the vector data has a dimension of $d$, LFDA has to solve the eigenvalue a matrix with dimension $d\times d$. In some descriptors the $d$ could be more than 20000 and thus the computation cost is intractable. 
 
\subsection{Kernel local fisher discriminant analysis(KLFDA)}

KLFDA  \cite{KLFDA} is the nonlinear version of LFDA. Most dimensionality reduction methods including PCA, LDA and LFDA are linear dimensionality reduction methods. However, when descriptors data are non-linear in feature space, its hard to capture its between-class discriminant information with linear reduction methods. One alternative method is to nonlinearly map input descriptors $\bm{x}_i$ to higher dimensional feature space $\Phi$ by a function $\phi(\bm{x}_i)$, again the LFDA is performed in feature space $\Phi$. Thus the transformation matrix $\bm{T}$ can be computed by equation
\begin{equation}
\bm{T} = \arg \min \frac{\bm{T}^T\bm{S}^{(b)}_{\phi}\bm{T}}{\bm{T}^T\bm{S}^{(w)}_{\phi}\bm{T}}
\end{equation}
where $\bm{S}^{(b)}_{\phi}$ and $\bm{S}^{(w)}_{\phi}$ is the between class scatter and within class scatter in mapped feature space $\Phi$.

Note that the transformation matrix $\bm{T} \in \Phi$, it's computationally expensive to explicitly compute the mapping function $\phi$ and perform LFDA in feature space $\Phi$ because the dimension of $\Phi$ may be infinite. Rather than explicitly computing, the mapping function $\phi$ can be implicit and the feature space $\Phi$ can be defined by the inner product of features in $\Phi$. Kernel trick is used here and a kernel function can be defined as the inner product of mapped vectors $\phi(\bm{x}_i)$ and $\phi(\bm{x}_j)$ by equation
 \begin{equation}
 k(\bm{x}_i,\bm{x}_j) = <\phi(\bm{x}_i),\phi(\bm{x}_j)>,
 \end{equation}
 the $< \cdot >$ is the inner product. There are many kinds of kernel like linear kernel, polynomial kernel and radial basis function (RBF) kernel. In this paper the RBF kernel is adopted. A RBF kernel is defined as 
 \begin{equation}
 k_{RBF}(\bm{x}_i,\bm{x}_j) = \exp^{(-\gamma||\bm{x}_i-\bm{x}_j||^2)}. 
 \end{equation}
Suppose $\bm{X}$ is the sample descriptors matrix, and we have
\begin{equation}
\bm{X} = (\bm{x}_1, \bm{x}_2,\cdots, \bm{x}_n), 
\end{equation}
and the label vector is $\bm{l} = (l_1, l_2, \cdots, l_n)$. Then the kernel matrix of $\bm{X}$ can be computed as following equation:
\begin{equation}
\bm{K} =  \phi(\bm{X})^T \phi(\bm{X})
\end{equation}
and we have 
\begin{equation}
\bm{K}_{i,j} =  k(\bm{x}_i,\bm{x}_j) = <\phi(\bm{x}_i),\phi(\bm{x}_j)> =  \exp^{(-\gamma||\bm{x}_i-\bm{x}_j||^2)}
\end{equation}

In \cite{KLFDA} the authors proposed fast computation of LFDA by replacing $\bm{S}^{(b)}$ with the local scatter mixture matrix $\bm{S}^{(m)}$defined by 
\begin{equation}
\begin{aligned}
\bm{S}^{(m)} &= \bm{S}^{(b)} + \bm{S}^{(w)}\\
\bm{S}^{(m)} &= \frac{1}{2} \sum_{i,j = 1} \bm{A}_{i,j}^{(m)} (\bm{x}_i - \bm{x}_j)(\bm{x}_i - \bm{x}_j)^T
\end{aligned}
\end{equation}
and 

\begin{equation}
\bm{A}_{i,j}^{(m)} = \bm{A}_{i,j}^{(w)}  + \bm{A}_{i,j}^{(w)}
\end{equation}

\noindent according to indentify(Fukunaga, 1990)
\begin{equation}
trace((\bm{T}^T\bm{S}^{(w)}\bm{T})^{(-1)}(\bm{T}^T\bm{S}^{(m)}\bm{T}) = tr((\bm{T}^T\bm{S}^{(w)}\bm{T})^{(-1)}(\bm{T}^T\bm{S}^{(b)}\bm{T}) + m
\end{equation}
equation \ref{eigencompute1} is equal to 

\begin{equation}
\bm{T}_{LFDA}  = \arg\min_{\bm{T}} (\frac{\bm{T}^T\bm{S}^{(m)}\bm{T}}{\bm{T}^T\bm{S}^{(w)}\bm{T}})
\end{equation}
and it can be transformed into a eigenvalue decomposition problem 
\begin{equation}\label{eigen2}
\bm{S}^{(m)}\bm{t}_i  = \lambda \bm{S}^{(w)}\bm{t}_i
\end{equation}
Also with the replacement of $\bm{S}^{(m)}$, in \cite{KLFDA} the author summarized that 
\begin{equation}
\bm{S}^{(m)} = \bm{X}\bm{L}^{(m)}\bm{X}^T
\end{equation}
where $\bm{L}^{(m)}  = \bm{D}^{(m)} - \bm{A}^{(m)}$, and $ \bm{D}^{i,i} = \sum_{j=1}^n  \bm{A}_{i,j}^{(m)}$. Also $\bm{S}^{(w)}$ can be represented as 
\begin{equation}
\bm{S}^{(w)} = \bm{X}\bm{L}^{(w)}\bm{X}^T
\end{equation}
where $\bm{L}^{(w)}  = \bm{D}^{(w)} - \bm{A}^{(m)}$, and $ \bm{D}^{i,i} = \sum_{j=1}^n  \bm{A}_{i,j}^{(w)}$. 
Therefore, equation \ref{eigen2} can be represented as
\begin{equation}\label{equationS}
\bm{X}\bm{L}^{(m)}\bm{X}^T \bm{t}_i= \lambda\bm{X}\bm{L}^{(w)}\bm{X}^T \bm{t}_i
\end{equation}
the eigen vector $\bm{t}_i$  can be represented as $\bm{t}_i = \bm{X}\bm{\gamma}$, vector $\bm{\gamma}_i \in R^n$, with this replacement, we left multiply $\bm{X}^T$ to equation \ref{equationS} to get 
\begin{equation}
\bm{X}^T\bm{X}\bm{L}^{(m)}\bm{X}^T\bm{X}\gamma_i = \lambda\bm{X}^T\bm{X}\bm{L}^{(w)}\bm{X}^T \bm{X}\gamma_i
\end{equation}
and by the kernel trick, its represented as
\begin{equation}
\bm{K}\bm{L}^{(m)}\bm{K}\gamma_i  = \lambda \bm{K}\bm{L}^{(w)}\bm{K}\gamma_i
\end{equation}
One example of using KLFDA to reduce dimension and classify the nonlinear data clusters can be shown in Figures \ref{KLFDAdemo1}, \ref{KLFDAdemo2} and \ref{KLFDAdemo3}. Three classes with five clusters are distributed on a 2-D plane, by KLFDA dimension reduction its 1-D dimension reduced data distribution are shown in Figure \ref{KLFDAdemo2} and Figure \ref{KLFDAdemo3}. It shows that for those clusters the gaussian kernel are better than linear kernel because the dimensionality reduced data are more separate when using gaussian kernel function.

\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{/Users/JohnsonJohnson/Downloads/thesis_1/Figures/KLFDAdemo1.jpg}
\caption{Example of five clusters belong to three classes}
\label{KLFDAdemo1}
\vspace{0em}
\end{figure} 


\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{/Users/JohnsonJohnson/Downloads/thesis_1/Figures/KLFDAdemo2.jpg}
\caption{1-D distribution of dimension reduced data  with gaussian kernel}
\label{KLFDAdemo2}
\vspace{-1em}
\end{figure} 


\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{/Users/JohnsonJohnson/Downloads/thesis_1/Figures/KLFDAdemo3.jpg}
\caption{1-D distribution of dimension reduced data  with linear kernel}
\label{KLFDAdemo3}
\vspace{-1em}
\end{figure} 

%\begin{figure}[H]
%\centering
%\begin{minipage}[t]{0.5\linewidth}
%\includegraphics[width=2.7in]{/Users/JohnsonJohnson/Downloads/thesis_1/Figures/KLFDAdemo2.jpg}
%%\caption{RGB patch}
%\label{fig:side:a}
%\end{minipage}%
%\begin{minipage}[t]{0.5\linewidth}
%\centering
%\includegraphics[width=2.7in]{/Users/JohnsonJohnson/Downloads/thesis_1/Figures/KLFDAdemo3.jpg}
%%\caption{GBR patch}
%\label{fig:side:b}
%\end{minipage}
%\caption{A comparison of two patches with same entropy but different color distribution}
%\end{figure}

\section{Mahalanobis distance}
The Mahalanobis distance \cite{Mahadist} based metric learning has received much attention in similarity computing. The Mahanalobis distance of two observations $\bm{x} $ and $\bm{y}$ is defined as
\begin{equation}
D(\bm{x},\bm{y}) = (\bm{x} - \bm{y})^T\bm{M}(\bm{x} - \bm{y}), 
\end{equation}
where $\bm{x}$ and $\bm{y} $ are $d\times1$ observation vectors, $\bm{M}$ is a semi-positive definite matrix. Since $\bm{M}$ is a SPD matrix, $\bm{M}$ can be decomposed as $\bm{M} = \bm{W}^T\bm{W}$, and Mahanalobis distance can also be written as 
\begin{equation}
D(\bm{x},\bm{y}) = (\bm{x} - \bm{y})^T\bm{W}^T\bm{W}(\bm{x} - \bm{y})= ||\bm{W}(\bm{x} - \bm{y})||
\end{equation}
 Therefore, Mahanalobis distance can be regarded as a variant of Euclidean distance.
 
 \section{Gradient descent optimization}
 Given a multivariate function $F(\bm{x})$, $\bm{x}$ is a $d$ dimensional vector, if $f(\bm{x})$ is continuous and differentiable in the neighbour of point $\bm{x}$ for all $\bm{x}$, then $f(\bm{x})$ decreases fastest in the direction of negative gradient of $F$ at $\bm{x}$. To compute the minimum of $F(\bm{x})$, an iterative method can be used by updating $F$ with respect to $\bm{x}$. If the updating step $\lambda$ is small enough, by updating $\bm{x}$ with 
 \begin{equation}
 \bm{x}_{t+1} = \bm{x}_{t} - \lambda \bm{G}
 \end{equation}
we have 
  \begin{equation}
 F(\bm{x}_{t+1}) \ge F(\bm{x}_t).
  \end{equation}
This process is repeated until certain condition is met, generally when gradient $||\bm{G}|| \le \eta$, $\eta$ is a very small positive integer. 
\begin{figure}
\centering
\includegraphics[width=0.4\linewidth]{/Users/JohnsonJohnson/Downloads/thesis_1/Figures/Gradientdescent.png}
\caption{Steepest gradient descent}
\vspace{0em}
\end{figure} 

\textbf{Analysis of steepest gradient descent method} The advantages of gradient descent are it's always downhill and it can avoid the saddle points. Besides, it's very efficient when initial value of $F(\bm{x})$ is further from minimum. However, there are a few shortcomings of gradient descent method. The first one is the convergence value of gradient descent might be the local minima of $F(\bm{x})$ if $F(\bm{x})$ is not monotonic (an example is in Figure \ref{fig:side:a}). In this case the convergence value will depend on the initial value of $\bm{x}$. Another shortcoming is the converging speed goes very slow when approaching the minimum. One example of slow approaching speed is the zigzag approaching case in Figure \ref{fig:side:b}. The third shortcoming is linear search in gradient descent might cause some problem.
\begin{figure}[H]
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=2.2in]{/Users/JohnsonJohnson/Downloads/thesis_1/Figures/multiMinima.png}
\caption{Function with multi local minimums}
\label{fig:side:a}
\end{minipage}%
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=2.2in]{/Users/JohnsonJohnson/Downloads/thesis_1/Figures/zigzag.jpg}
\caption{Zigzagging downhill valley}
\label{fig:side:b}
\end{minipage}
\end{figure}

 \section{Metric learning based on sample pairs distance comparison}
 Inspired by \cite{TDL}, in this paper, a similar metric learning based on iteration computation is used. For a  sample descriptor $\bm{x}_i$,  its positive pairwise set is defined as $\{\bm{x}_i,\bm{x}_j\}$, where class ID $y_i = y_j$. Also the negative pairwise set can be defined as $\{\bm{x}_i,\bm{x}_j\}$, where $y_i \ne y_j$. Similar with \cite{PRDC}, this method is also based on similarity comparison. The difference is in \cite{PRDC}, for all possible positive and negative pairs, the distance between positive pairs must be smaller than the distance between negative pairs. Since it has to compare all possible positive and negative pairs, its computation complexity is quite expensive. To decrease complexity, a simplified version is proposed as the top-push distance metric learning \cite{TDL}.  Since re-identification is a problem of ranking, it is desired that the rank-1 descriptor should be the right match. Given a Mahanalobis matrix $\bm{M}$, for samples $\bm{x}_i, i = 1,2,3,\cdots,n$, $n$ is the number of all samples, the requirement is distance between positive pair should be at least 1 unit smaller than the minimum distance of all negative pair. This can be denoted as 
 \begin{equation}
 D(\bm{x}_i,\bm{x}_j) + \rho < \min D(\bm{x}_i,\bm{x}_k), y_i = y_j, y_i\ne y_k.
 \end{equation}
 $\rho$ is a slack variable and $\rho \in [0,1]$. This equation can be transformed into a optimization problem as
 \begin{equation}
 \label{term1}
 \min \sum_{y_i = y_j} \max \{D(\bm{x}_i,\bm{x}_j) -  \min_{ y_i\ne y_k} D(\bm{x}_i,\bm{x}_k)  + \rho , 0\}.
 \end{equation}
However, the equation above only penalizes the minimum interclass distance. Another term is needed to penalize large intraclass distance. That is, the sum of intraclass distance should be as small as possible. This term is denoted as 
 \begin{equation} \label{term2}
 \min \sum_{y_i = y_j} D(\bm{x}_i,\bm{x}_j).
 \end{equation}

 
 To combine equations above, a ratio factor $\alpha$ is assigned to equation \eqref{term1} and \eqref{term2} so that the target function can be denoted as 
  \begin{equation}
  \begin{aligned}
 f(\bm{M}) = (1-\alpha)\sum_{\bm{x}_i,x_j,\bm{y}_i=y_j} D(\bm{x}_i,\bm{x}_j) + \\
  \alpha \sum_{\bm{x}_i,\bm{x}_j,y_i=y_j}\max\{{D(\bm{x}_i,\bm{x}_j)-\min_{y_i\ne y_k}{D(\bm{x}_i,\bm{x}_k)}+\rho,0}\}
 \end{aligned}
 \end{equation}
 In this way the problem is transformed to an optimization problem. Notice that equation 16 can be denoted as 
 \begin{equation}
 D(\bm{x},\bm{y}) = (\bm{x} - \bm{y})^T\bm{M}(\bm{x} - \bm{y}) = trace(\bm{M}\bm{X}_{i,j})
 \end{equation}
 where $\bm{X}_{i,j} = \bm{x}_i*\bm{x}_j^T$, and $trace$ is to compute matrix trace. Therefore, equation 21 can be transformed as follow,
 \begin{equation}
 \begin{aligned}
 f(\bm{M}) = (1-\alpha)\sum_{y_i = y_j}trace(\bm{M}\bm{X}_{i,j}) \\
  + \alpha \sum_{y_i = y_j,y_i\ne y_k}\max\{trace(\bm{M}\bm{X}_{i,j}) - trace(\bm{M}\bm{X}_{i,k} )+ \rho,0\}
 \end{aligned}
 \end{equation}
 
 To minimize equation 23, the gradient descent method is used. The gradient respect to $\bm{M}$ is computed as
 \begin{equation}
 \begin{aligned}
\bm{G} =  \frac{\partial f}{\partial \bm{M}} = (1-\alpha) \sum_{y_i = y_j} \bm{X}_{i,j} 
 + \alpha \sum_{y_i = y_j, y_i \ne y_k}(\bm{X}_{i,j} - \bm{X}_{i,k})
 \end{aligned}
 \end{equation}
 
 The iteration process can be summarized as in Table \ref{Gradientdemo}.
% \begin{table}[H]
% \caption{The optimization algorithm for metric learning}
% \centering
% \begin{tabular}{l}
% \hline 
% \multicolumn{1}{l}{\textbf{Gradient optimization algorithm for target function}} \\
% \hline
% \textbf{Input} Descriptors of training person pairs \\
% \textbf{Output} A SPD matrix\\
% \textbf{Initialization} \\
% Initialize $\bm{M}$ with eye matrix $\bm{I}$; \\
% Compute the initial target function value $f_0$ with $\bm{M}_0$;\\
% Iteration count  $t = 0$;\\
%
% \textbf{while}(not converge)\\
% \space Update $t =  t + 1$;\\
% \indent Update gradient $\bm{G}_{t+1}$ with equation 24;\\
% \indent Update $\bm{M}$ with equation : $\bm{M}_{t+1} = \bm{M}_{t} - \lambda\bm{G}_t$\\
% \indent Project $\bm{M}_{t+1}$ to the positive semi-definite space \\ 
% \indent \indent by $\bm{M}_{t+1}= \bm{V}_{t+1}\bm{S}_{t+1}\bm{V}^T_{t+1}$;\\
% \indent Update the target value $f|_{\bm{M} = \bm{M}_{t+1}}$;\\
% \textbf{end while}  \\
% return $\bm{M}$\\
% \hline
% \end{tabular} 
% \end{table}

\begin{table}[H] 

 \centering
 \caption{Optimization algorithm of Mahanalobis distance matrix learning}
 \label{Gradientdemo}
 \begin{tabular}{l}
 \hline 
 \multicolumn{1}{l}{\textbf{Gradient optimization algorithm for target function}} \\
 \hline
 \textbf{Input} Descriptors of training person pairs \\
 \textbf{Output} A SPD matrix\\
 \textbf{Initialization} \\
 Initialize $\bm{M}_0$ with eye matrix $\bm{I}$; \\
 Compute the initial target function value $f_0$ with $\bm{M}_0$;\\
 Iteration count  $t = 0$;\\

 \textbf{while}(not converge)\\
 \hspace{0.5cm} Update $t =  t + 1$;\\
 \hspace{0.5cm}  Find $\bm{x}_k$ for all sample points $\bm{x}_i$, where $y_i \ne y_k$;\\
 \hspace{0.5cm} Update gradient $\bm{G}_{t+1}$ with equation 12;\\
 \hspace{0.5cm}  Update $\bm{M}$ with equation : $\bm{M}_{t+1} = \bm{M}_{t} - \lambda\bm{G}_t$;\\
 \hspace{0.5cm}  Project $\bm{M}_{t+1}$ to the positive semi-positive definite space; \\ 
% \indent \indent by $\bm{M}_{t+1}= \bm{V}_{t+1}\bm{S}_{t+1}\bm{V}^T_{t+1}$;\\
 \hspace{0.5cm}  Update the target value $f|_{\bm{M} = \bm{M}_{t+1}}$;\\
 \textbf{end while}  \\
 return $\bm{M}$\\
 \hline
 \end{tabular} 
 \end{table}

In each iteration, to make sure the updated $\bm{M}$ is a SPD matrix, first a eigenvalue decomposition is performed on $\bm{M}$, and we have
\begin{equation}\label{SPDproject}
\bm{M} = \bm{V}\bm{\Lambda}\bm{V}^T
\end{equation}
here $\bm{\Lambda}$ is a diagonal matrix and its diagonal elements are eigenvalues. Then the negative eigenvalues in $\bm{V}$ are removed and the corresponding eigenvectors in $\bm{V}$ are also removed. Then $\bm{M}$ is restored by equation \eqref{SPDproject}.

 
 
 
 
 
